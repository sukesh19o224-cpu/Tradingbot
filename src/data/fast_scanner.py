"""
‚ö° FAST SCANNER - Multi-threaded Stock Scanning
Scan 200+ stocks in 2-4 minutes instead of 10-15 minutes
"""

import concurrent.futures
from typing import Dict, List
import time
from datetime import datetime

from src.data.data_fetcher import DataFetcher
from src.strategies.signal_generator import SignalGenerator
from config.settings import *


class FastScanner:
    """
    Multi-threaded scanner for processing many stocks quickly

    Features:
    - Parallel data fetching (10x faster)
    - Batch processing
    - Progress tracking
    - Error resilience
    """

    def __init__(self, max_workers: int = 10):
        self.data_fetcher = DataFetcher()
        self.signal_generator = SignalGenerator()
        self.max_workers = max_workers  # Number of parallel threads

    def scan_all_stocks(self, symbols: List[str]) -> Dict:
        """
        Scan all stocks with multi-threading

        Args:
            symbols: List of stock symbols

        Returns:
            Dict with stock_data and signals
        """
        print(f"\n{'='*70}")
        print(f"‚ö° FAST MULTI-THREADED SCANNER")
        print(f"{'='*70}")
        print(f"üìä Stocks to scan: {len(symbols)}")
        print(f"üîß Parallel threads: {self.max_workers}")
        print(f"‚è∞ Started: {datetime.now().strftime('%H:%M:%S')}")
        print(f"{'='*70}\n")

        start_time = time.time()

        # Step 1: Fetch data in parallel
        print("üì° Step 1/2: Fetching market data (parallel)...")
        stock_data = self._fetch_data_parallel(symbols)

        fetch_time = time.time() - start_time
        print(f"‚úÖ Data fetched: {len(stock_data)}/{len(symbols)} stocks in {fetch_time:.1f}s\n")

        # Step 2: Generate signals
        print("‚ö° Step 2/2: Generating signals...")
        signal_start = time.time()

        signals = self.signal_generator.scan_multiple_stocks(
            list(stock_data.keys()),
            stock_data
        )

        signal_time = time.time() - signal_start
        total_time = time.time() - start_time

        # Summary
        print(f"\n{'='*70}")
        print(f"‚úÖ SCAN COMPLETE")
        print(f"{'='*70}")
        print(f"‚è±Ô∏è  Total Time: {total_time:.1f}s")
        print(f"   ‚îî‚îÄ Data Fetch: {fetch_time:.1f}s")
        print(f"   ‚îî‚îÄ Signal Gen: {signal_time:.1f}s")
        print(f"üìä Success Rate: {len(stock_data)}/{len(symbols)} ({len(stock_data)/len(symbols)*100:.1f}%)")
        print(f"üéØ Signals Found: {len(signals)} (score >= {MIN_SIGNAL_SCORE})")
        print(f"‚ö° Speed: {len(symbols)/total_time:.1f} stocks/second")
        print(f"{'='*70}\n")

        return {
            'stock_data': stock_data,
            'signals': signals,
            'stats': {
                'total_stocks': len(symbols),
                'successful_fetches': len(stock_data),
                'signals_generated': len(signals),
                'total_time': total_time,
                'fetch_time': fetch_time,
                'signal_time': signal_time
            }
        }

    def _fetch_data_parallel(self, symbols: List[str]) -> Dict:
        """
        Fetch stock data using multiple threads

        This is 10x faster than sequential fetching!
        """
        stock_data = {}
        failed = []

        def fetch_single(symbol):
            """Fetch data for a single stock"""
            try:
                df = self.data_fetcher.get_stock_data(symbol)
                return (symbol, df)
            except Exception as e:
                return (symbol, None)

        # Use ThreadPoolExecutor for parallel fetching
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_symbol = {
                executor.submit(fetch_single, symbol): symbol
                for symbol in symbols
            }

            # Progress tracking
            completed = 0
            total = len(symbols)

            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1

                try:
                    symbol, df = future.result()

                    if df is not None and not df.empty:
                        stock_data[symbol] = df
                        status = "‚úÖ"
                    else:
                        failed.append(symbol)
                        status = "‚ùå"

                    # Progress indicator (every 10%)
                    if completed % max(1, total // 10) == 0:
                        progress = (completed / total) * 100
                        print(f"   Progress: {completed}/{total} ({progress:.0f}%) - Latest: {symbol} {status}")

                except Exception as e:
                    failed.append(symbol)
                    print(f"   ‚ö†Ô∏è Error fetching {symbol}: {str(e)[:50]}")

        if failed:
            print(f"\n‚ö†Ô∏è Failed to fetch: {len(failed)} stocks")
            if len(failed) <= 10:
                print(f"   {', '.join(failed)}")

        return stock_data


def test_fast_scanner():
    """Test the fast scanner"""
    print("üß™ Testing Fast Scanner...\n")

    # Test with first 50 stocks
    from config.nse_universe import NIFTY_200

    test_symbols = NIFTY_200[:50]

    scanner = FastScanner(max_workers=10)
    result = scanner.scan_all_stocks(test_symbols)

    print(f"\nüìä Test Results:")
    print(f"   Stocks scanned: {result['stats']['total_stocks']}")
    print(f"   Success rate: {result['stats']['successful_fetches']}/{result['stats']['total_stocks']}")
    print(f"   Signals: {result['stats']['signals_generated']}")
    print(f"   Time: {result['stats']['total_time']:.1f}s")
    print(f"   Speed: {result['stats']['total_stocks']/result['stats']['total_time']:.1f} stocks/sec")

    if result['signals']:
        print(f"\nüéØ Top Signal:")
        top = result['signals'][0]
        print(f"   {top['symbol']}: {top['score']}/10 ({top['trade_type']})")


if __name__ == "__main__":
    test_fast_scanner()
